{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do:\n",
    "1. Текст был очищен только от одного мусорного элемента в качестве примера. Исслудйте данные через ноутбук или чере веб-интерфейс BigQuery на предмет других мусорных элементов в тексте, которые не несут в себе никакого особого смысла, а только создают шум в данных. Доработайте функцию очистки тектосвых данных, чтобы в нее можно было передать список ненужного мусора и разом выполнялась очистка\n",
    "2. Проведите стратифицировнную кросс-валидуцию нейросетевого классификатора https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "3. Поэксперементируйте с гиперпараметрами нейросетевого классификатора, постарайтесь повысить качество его работы\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "4. Попробуйте использовать не Word2Vec для получения векторого представления текста, а TF-IDF преобразование http://zabaykin.ru/?p=558 http://nlpx.net/archives/57\n",
    "5. Попробуйте использовать более тонко настриваемые алгоритмы нейросетей, например из этого видео https://www.youtube.com/watch?v=cPkH1k3U1c8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "import pandas_gbq \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as mt\n",
    "import datetime as dt\n",
    "\n",
    "from langdetect import detect\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv(dotenv.find_dotenv('.env'))\n",
    "\n",
    "CREDENTIALS = service_account.Credentials.from_service_account_info({\n",
    "    \"type\": os.getenv(\"TYPE\"),\n",
    "    \"project_id\": os.getenv(\"PROJECT_ID\"),\n",
    "    \"private_key_id\": os.getenv(\"PRIVATE_KEY_ID\"),\n",
    "    \"private_key\": os.getenv(\"PRIVATE_KEY\"),\n",
    "    \"client_email\": os.getenv(\"CLIENT_EMAIL\"),\n",
    "    \"client_id\": os.getenv(\"CLIENT_ID\"),\n",
    "    \"auth_uri\": os.getenv(\"AUTH_URI\"),\n",
    "    \"token_uri\": os.getenv(\"TOKEN_URI\"),\n",
    "    \"auth_provider_x509_cert_url\": os.getenv(\"AUTH_PROVIDER_X509_CERT_URL\"),\n",
    "    \"client_x509_cert_url\": os.getenv(\"CLIENT_X509_CERT_URL\"),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion for getting fresh data from DWH for workload model\n",
    "\"\"\"[summary]\n",
    "Funtion for getting fresh data from BigQuery for workload scoring model\n",
    "[description]\n",
    "Credentials - google service account object with credentials data for project\n",
    "[example]\n",
    "Input: Credentials = credentials_object\n",
    "Output: description\t                                        channel\t category\tcategory_flag\n",
    "        \\nChat transcript:\\nVisitor: I want to buy wit...\tchat\t ps\t        1\n",
    "        \\nChat transcript:\\nVisitor: hell i had a prob...\tchat\t ps\t        1\n",
    "        \\nChat transcript:\\nVisitor: لا استطيع الشراء ...\t chat\t  ps\t     1\n",
    "\"\"\"\n",
    "def getDwhData(Credentials):\n",
    "    statement_bigquery_sql = \" \".join([\"select description, channel, case\",\n",
    "                                       \"when manual_category in ('payment_problem','how_to_pay','howtopay','how_to_play','paystation_error','ps_problem','ps_declined') then 'ps'\",\n",
    "                                       \"else 'other'\",\n",
    "                                       \"end as category,\",\n",
    "                                       \"case\",\n",
    "                                       \"when manual_category in ('payment_problem','how_to_pay','howtopay','how_to_play','paystation_error','ps_problem','ps_declined') then 0\",\n",
    "                                       \"else 1\",\n",
    "                                       \"end as category_flag\",\n",
    "                                       \"from `xsolla_summer_school.customer_support`\",\n",
    "                                       \"where manual_category is not null and\",\n",
    "                                       \"manual_category <> '' and\",\n",
    "                                       \"description is not null and\",\n",
    "                                       \"description <> '' and\",\n",
    "                                       \"channel is not null and\",\n",
    "                                       \"channel <> '' and\",\n",
    "                                       \"channel in ('chat','facebook')\"])\n",
    "    \n",
    "    dataframe_bigquery = pandas_gbq.read_gbq(statement_bigquery_sql,project_id='findcsystem', credentials=Credentials, dialect='standard')\n",
    "\n",
    "    return dataframe_bigquery\n",
    "\n",
    "\n",
    "\"\"\"[summary]\n",
    "Function for transform text to lower case\n",
    "[description]\n",
    "Corpus - list or array object, with text data\n",
    "[example]\n",
    "Input: Corpus = [\"Text_1\",\"Text_2\"]\n",
    "Output: [\"text_1\",\"text_2\"]\n",
    "\"\"\"\n",
    "def lowerCase(Corpus):\n",
    "    corpus = [i.lower().replace('\\n',' ') for i in Corpus]\n",
    "    return corpus\n",
    "\n",
    "\n",
    "\"\"\"[summary]\n",
    "Function for getting language of text\n",
    "[description]\n",
    "Corpus - list or array object, with text data\n",
    "[example]\n",
    "Input: Corpus = [\"Text_1\",\"Text_2\"]\n",
    "Output: [\"en\",\"ru\"]\n",
    "\"\"\"\n",
    "def getTextLanguage(Corpus):\n",
    "    txt_lang = []\n",
    "    for txt in Corpus:\n",
    "        try:\n",
    "            lang = detect(txt)\n",
    "        except Exception as e:\n",
    "            lang = 'error'\n",
    "        finally:\n",
    "            txt_lang.append(lang)\n",
    "    \n",
    "    return txt_lang\n",
    "\n",
    "\n",
    "\"\"\"[summary]\n",
    "Function for tokenization text\n",
    "[description]\n",
    "Corpus - list or array object, with text data\n",
    "[example]\n",
    "Input: Corpus = [\"word1 word2\",\"word3 word4\"]\n",
    "Output: [[\"word1\",\"word2\"],[\"word3\",\"word4\"]]\n",
    "\"\"\"  \n",
    "def textToTokens(Corpus):\n",
    "    corpus = [i.split() for i in Corpus]\n",
    "    return corpus \n",
    "\n",
    "\n",
    "\"\"\"[summary]\n",
    "Function for clear text after garbage\n",
    "[description]\n",
    "Corpus - list or array object, with text data\n",
    "Substr - string, regular expression\n",
    "[example]\n",
    "Input: Corpus = [[\"word1\",\"word2\"],[\"word3\",\"word4\"]]\n",
    "       Substr = r'word1\n",
    "Output: [[\"word2\"],[\"word3\",\"word4\"]]\n",
    "\"\"\"  \n",
    "def clearTextAfterGarbage(Corpus,Substr):\n",
    "    return [[word for word in text if Substr not in word] for text in Corpus]\n",
    "\n",
    "\n",
    "\"\"\"[summary]\n",
    "Function for replace urls\n",
    "[description]\n",
    "Corpus - list or array object, with text data\n",
    "[example]\n",
    "Input: Corpus = [[\"http://some.url.ru/\",\"word2\"],[\"word3\",\"word4\"]]\n",
    "Output: [[\"HTTP\", \"word2\"],[\"word3\",\"word4\"]]\n",
    "\"\"\"  \n",
    "def clearTextFromUrls(Corpus):\n",
    "    return [[re.sub(\"(^https?:\\/\\/.*)|(.*\\.com.*)\", \"HTTPADDRESS\", word) for word in text] for text in Corpus]\n",
    "\n",
    "\n",
    "\"\"\"[summary]\n",
    "Function for clear text from short words\n",
    "[description]\n",
    "Corpus - list or array object, with text data\n",
    "WordLength - int, length having which word will be delete\n",
    "[example]\n",
    "Input: Corpus = [[\"word1\",\"word1234\"],[\"word123\",\"word1234\"]]\n",
    "       WordLength = 6\n",
    "Output: [[\"word1234\"],[\"word123\",\"word1234\"]]\n",
    "\"\"\"  \n",
    "def clearShortWordsAsGarbage(Corpus, WordLength):\n",
    "    return [[word for word in text if len(word) > WordLength] for text in Corpus]\n",
    "\n",
    "\n",
    "\"\"\"[summary]\n",
    "Function for clear from non-english text\n",
    "[description]\n",
    "Corpus - list or array object, with text data\n",
    "[example]\n",
    "Input: Corpus = [[\"word\",\"table1\"],[\"table\",\"word1234\"]]\n",
    "Output: [[\"word\"],[\"table\"]]\n",
    "\"\"\"  \n",
    "def clearNonEnglish(Corpus):\n",
    "    EN_ALPHABET = \"qwertyuiopasdfghjklzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM\"\n",
    "    return [[word for word in text if all(char in EN_ALPHABET for char in word)] for text in Corpus]\n",
    "\n",
    "\n",
    "\"\"\"[summary]\n",
    "Function for clear words from punctuation\n",
    "[description]\n",
    "Corpus - list or array object, with text data\n",
    "[example]\n",
    "Input: Corpus = [[\"word,\",\"table!\"],[\"table..\",\"word1234\"]]\n",
    "Output: [[\"word,\",\"table\"],[\"table\",\"word1234\"]]\n",
    "\"\"\"  \n",
    "def clearPunctuation(Corpus):\n",
    "    return [[re.sub(r\"[^a-zA-Z]*\", \"\", word) for word in text if re.search(\"[a-zA-Z]\", word)] for text in Corpus]\n",
    "\n",
    "\n",
    "\"\"\"[summary]\n",
    "Build word vector by using pre-trained Word2Vec model\n",
    "[description]\n",
    "Size - lenght of vector\n",
    "Word2Vec_Model - gensim object\n",
    "\"\"\"  \n",
    "def buildWordVector(Text,Size,Word2Vec_Model):\n",
    "    vec = np.zeros(Size).reshape((1,Size))\n",
    "    count = 0.\n",
    "\n",
    "    for word in Text:\n",
    "        try:\n",
    "            vec += Word2Vec_Model[word].reshape((1,Size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAWDATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23450, 5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "#getting data from dwh\n",
    "if os.path.isfile('neuro.csv'):\n",
    "    SupportRawDataframe = pd.read_csv('neuro.csv')\n",
    "else:\n",
    "    SupportRawDataframe = getDwhData(CREDENTIALS)\n",
    "    SupportRawDataframe.to_csv('neuro.csv')\n",
    "SupportRawDataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform text to lower case\n",
    "corpus = SupportRawDataframe.description\n",
    "corpus.astype('str')\n",
    "\n",
    "corpus = lowerCase(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting language for text corpus\n",
    "corpus_lang = getTextLanguage(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new dataframe with texts in lower case, without /n symbol and with lang for text\n",
    "SupportRawDataframe['description'] = corpus\n",
    "SupportRawDataframe['lang'] = corpus_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>description</th>\n",
       "      <th>channel</th>\n",
       "      <th>category</th>\n",
       "      <th>category_flag</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>chat transcript: visitor: i want to buy with ...</td>\n",
       "      <td>chat</td>\n",
       "      <td>ps</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>chat transcript: visitor: hell i had a proble...</td>\n",
       "      <td>chat</td>\n",
       "      <td>ps</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>chat transcript: visitor: لا استطيع الشراء وم...</td>\n",
       "      <td>chat</td>\n",
       "      <td>ps</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>chat transcript: visitor: im having trouble w...</td>\n",
       "      <td>chat</td>\n",
       "      <td>ps</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>chat transcript: visitor: hi ana: hello. how ...</td>\n",
       "      <td>chat</td>\n",
       "      <td>ps</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23445</th>\n",
       "      <td>23445</td>\n",
       "      <td>chat transcript: visitor: hi, i made a prucha...</td>\n",
       "      <td>chat</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23446</th>\n",
       "      <td>23446</td>\n",
       "      <td>chat transcript: visitor: hi, how long will i...</td>\n",
       "      <td>chat</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23447</th>\n",
       "      <td>23447</td>\n",
       "      <td>chat transcript: visitor: i bought playerunkn...</td>\n",
       "      <td>chat</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23448</th>\n",
       "      <td>23448</td>\n",
       "      <td>chat transcript: visitor: good day i took the...</td>\n",
       "      <td>chat</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23449</th>\n",
       "      <td>23449</td>\n",
       "      <td>chat transcript: visitor: hi visitor: hello v...</td>\n",
       "      <td>chat</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16003 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                        description channel  \\\n",
       "0               0   chat transcript: visitor: i want to buy with ...    chat   \n",
       "1               1   chat transcript: visitor: hell i had a proble...    chat   \n",
       "2               2   chat transcript: visitor: لا استطيع الشراء وم...    chat   \n",
       "3               3   chat transcript: visitor: im having trouble w...    chat   \n",
       "4               4   chat transcript: visitor: hi ana: hello. how ...    chat   \n",
       "...           ...                                                ...     ...   \n",
       "23445       23445   chat transcript: visitor: hi, i made a prucha...    chat   \n",
       "23446       23446   chat transcript: visitor: hi, how long will i...    chat   \n",
       "23447       23447   chat transcript: visitor: i bought playerunkn...    chat   \n",
       "23448       23448   chat transcript: visitor: good day i took the...    chat   \n",
       "23449       23449   chat transcript: visitor: hi visitor: hello v...    chat   \n",
       "\n",
       "      category  category_flag lang  \n",
       "0           ps              0   en  \n",
       "1           ps              0   en  \n",
       "2           ps              0   en  \n",
       "3           ps              0   en  \n",
       "4           ps              0   en  \n",
       "...        ...            ...  ...  \n",
       "23445    other              1   en  \n",
       "23446    other              1   en  \n",
       "23447    other              1   en  \n",
       "23448    other              1   en  \n",
       "23449    other              1   en  \n",
       "\n",
       "[16003 rows x 6 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting only en texts\n",
    "SupportDataframe_eng = SupportRawDataframe[SupportRawDataframe.lang == 'en'][:]\n",
    "SupportDataframe_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text tekenization\n",
    "tokenization = textToTokens(SupportDataframe_eng.description)\n",
    "SupportDataframe_eng['description'] = tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>description</th>\n",
       "      <th>channel</th>\n",
       "      <th>category</th>\n",
       "      <th>category_flag</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[chat, transcript:, visitor:, i, want, to, buy...</td>\n",
       "      <td>chat</td>\n",
       "      <td>ps</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[chat, transcript:, visitor:, hell, i, had, a,...</td>\n",
       "      <td>chat</td>\n",
       "      <td>ps</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[chat, transcript:, visitor:, لا, استطيع, الشر...</td>\n",
       "      <td>chat</td>\n",
       "      <td>ps</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[chat, transcript:, visitor:, im, having, trou...</td>\n",
       "      <td>chat</td>\n",
       "      <td>ps</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[chat, transcript:, visitor:, hi, ana:, hello....</td>\n",
       "      <td>chat</td>\n",
       "      <td>ps</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23445</th>\n",
       "      <td>23445</td>\n",
       "      <td>[chat, transcript:, visitor:, hi,, i, made, a,...</td>\n",
       "      <td>chat</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23446</th>\n",
       "      <td>23446</td>\n",
       "      <td>[chat, transcript:, visitor:, hi,, how, long, ...</td>\n",
       "      <td>chat</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23447</th>\n",
       "      <td>23447</td>\n",
       "      <td>[chat, transcript:, visitor:, i, bought, playe...</td>\n",
       "      <td>chat</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23448</th>\n",
       "      <td>23448</td>\n",
       "      <td>[chat, transcript:, visitor:, good, day, i, to...</td>\n",
       "      <td>chat</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23449</th>\n",
       "      <td>23449</td>\n",
       "      <td>[chat, transcript:, visitor:, hi, visitor:, he...</td>\n",
       "      <td>chat</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16003 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                        description channel  \\\n",
       "0               0  [chat, transcript:, visitor:, i, want, to, buy...    chat   \n",
       "1               1  [chat, transcript:, visitor:, hell, i, had, a,...    chat   \n",
       "2               2  [chat, transcript:, visitor:, لا, استطيع, الشر...    chat   \n",
       "3               3  [chat, transcript:, visitor:, im, having, trou...    chat   \n",
       "4               4  [chat, transcript:, visitor:, hi, ana:, hello....    chat   \n",
       "...           ...                                                ...     ...   \n",
       "23445       23445  [chat, transcript:, visitor:, hi,, i, made, a,...    chat   \n",
       "23446       23446  [chat, transcript:, visitor:, hi,, how, long, ...    chat   \n",
       "23447       23447  [chat, transcript:, visitor:, i, bought, playe...    chat   \n",
       "23448       23448  [chat, transcript:, visitor:, good, day, i, to...    chat   \n",
       "23449       23449  [chat, transcript:, visitor:, hi, visitor:, he...    chat   \n",
       "\n",
       "      category  category_flag lang  \n",
       "0           ps              0   en  \n",
       "1           ps              0   en  \n",
       "2           ps              0   en  \n",
       "3           ps              0   en  \n",
       "4           ps              0   en  \n",
       "...        ...            ...  ...  \n",
       "23445    other              1   en  \n",
       "23446    other              1   en  \n",
       "23447    other              1   en  \n",
       "23448    other              1   en  \n",
       "23449    other              1   en  \n",
       "\n",
       "[16003 rows x 6 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SupportDataframe_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(SupportDataframe_eng.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"[summary]\n",
    "Attemp to automatize garbage selection\n",
    "[description]\n",
    "garbage - list of strings, init words to delete\n",
    "texts - list of strings, main texts\n",
    "\"\"\"  \n",
    "def train_function(garbage, texts):\n",
    "    tests_clear = texts\n",
    "    tests_clear = clearTextFromUrls(tests_clear)\n",
    "    tests_clear = clearPunctuation(tests_clear)\n",
    "    tests_clear = clearShortWordsAsGarbage(tests_clear, 3)\n",
    "    for garbage_word in garbage:\n",
    "        tests_clear = clearTextAfterGarbage(tests_clear, garbage_word)\n",
    "\n",
    "    SupportDataframe_eng['description'] = tests_clear\n",
    "\n",
    "    #list of unique categories\n",
    "    unique_categories = np.unique(SupportDataframe_eng.category)\n",
    "    descriptions = SupportDataframe_eng['description']\n",
    "    categories = SupportDataframe_eng['category_flag']\n",
    "    XTrain,XTest,YTrain,YTest = train_test_split(descriptions,\n",
    "                                                 categories,\n",
    "                                                 stratify = categories,\n",
    "                                                 test_size = 0.2,\n",
    "                                                 random_state = 40)\n",
    "\n",
    "    #initialize Word2Vec model for embedding words to vectors\n",
    "    NDim = 100\n",
    "    Imdb_w2v = Word2Vec(size = NDim,min_count = 10)\n",
    "    Imdb_w2v.build_vocab(XTrain)\n",
    "\n",
    "    Imdb_w2v.train(XTrain,total_examples = Imdb_w2v.corpus_count,epochs = Imdb_w2v.epochs)\n",
    "\n",
    "    #embedding training messages to vectors for neutral classifier\n",
    "    TrainVecs = np.concatenate([buildWordVector(i,NDim,Imdb_w2v) for i in XTrain])\n",
    "\n",
    "    Imdb_w2v.train(XTest, total_examples = Imdb_w2v.corpus_count, epochs = Imdb_w2v.epochs)\n",
    "    TestVecs = np.concatenate([buildWordVector(i,NDim,Imdb_w2v) for i in XTest])\n",
    "\n",
    "    TextClassifier = MLPClassifier(hidden_layer_sizes = (20,10), max_iter = 1000, random_state = 40)\n",
    "\n",
    "    Scores = cross_val_score(TextClassifier, TrainVecs, YTrain, cv = 5, n_jobs=-1)\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    result[\"mean_score\"] = np.mean(Scores)\n",
    "    \n",
    "    TextClassifier.fit(TrainVecs,YTrain)\n",
    "\n",
    "    pred = TextClassifier.predict(TestVecs)\n",
    "    result[\"conf_matrix\"] = confusion_matrix(YTest,pred)\n",
    "    result[\"class_report\"] = classification_report(YTest,pred)\n",
    "    \n",
    "    # select word correctly and incorrectly classified    \n",
    "    tp_index = YTest[(YTest == 1) & (pred == 1)].index\n",
    "    tn_index = YTest[(YTest == 0) & (pred == 0)].index\n",
    "    fn_index = YTest[(YTest == 1) & (pred == 0)].index\n",
    "    fp_index = YTest[(YTest == 0) & (pred == 1)].index\n",
    "\n",
    "    # count word amount in each group\n",
    "    bags = {\n",
    "        \"tp\": (tp_index, {}),\n",
    "        \"tn\": (tn_index, {}),\n",
    "        \"fp\": (fp_index, {}),\n",
    "        \"fn\": (fn_index, {}),\n",
    "    }\n",
    "    \n",
    "    for key, (index, _dict) in bags.items():\n",
    "        for sent in XTest[index]:\n",
    "            for word in sent:\n",
    "                if word not in _dict:\n",
    "                    _dict[word] = 0\n",
    "                _dict[word] += 1\n",
    "\n",
    "    # select top 10 words which cause false-positive result \n",
    "    fp_words_sorted = [word[0] for word in sorted(bags[\"fp\"][1].items(), key=lambda x: -x[1])]\n",
    "    \n",
    "    words_to_delete = set()\n",
    "    count_words_to_delete = 10\n",
    "    curr_count = 0\n",
    "    \n",
    "    for word in fp_words_sorted:\n",
    "        if word not in bags[\"tn\"][1]:\n",
    "            words_to_delete.add(word)\n",
    "            curr_count += 1\n",
    "        if curr_count == count_words_to_delete:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    # select top 10 words which cause false-negative result \n",
    "    fn_words_sorted = [word[0] for word in sorted(bags[\"fn\"][1].items(), key=lambda x: -x[1])]\n",
    "    \n",
    "    words_to_keep_in = set()\n",
    "    count_words_to_keep_in = 10\n",
    "    curr_count = 0\n",
    "    \n",
    "    for word in fn_words_sorted:\n",
    "        if word not in bags[\"tp\"][1]:\n",
    "            words_to_keep_in.add(word)\n",
    "            curr_count += 1\n",
    "        if curr_count == count_words_to_keep_in:\n",
    "            break\n",
    "    \n",
    "    # include in garbage fp-words and exclude fn-words\n",
    "    garbage |= words_to_delete\n",
    "    garbage -= words_to_keep_in\n",
    "    \n",
    "    return garbage, result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING! Executes for an hour+\n",
    "# WARNING! Executes for an hour+\n",
    "# WARNING! Executes for an hour+\n",
    "\n",
    "# to delete from text\n",
    "garbage = {'chat', 'transcript', 'visitor', } \n",
    "\n",
    "history = []\n",
    "\n",
    "for i in range(20):\n",
    "    print(f\"No: {i}\")\n",
    "    garbage, result = train_function(garbage, texts)\n",
    "    history.append((garbage, result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.52      0.35       635\n",
      "           1       0.84      0.65      0.73      2561\n",
      "\n",
      "    accuracy                           0.62      3196\n",
      "   macro avg       0.56      0.58      0.54      3196\n",
      "weighted avg       0.73      0.62      0.66      3196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(history[-1][1]['class_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.60'"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([i[1][\"class_report\"].split('\\n')[2].split()[3] for i in history])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Резульат 2+ часов: все больше мусорных слов выбирается и к концу падает точность"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN TEST SPLIT DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['other', 'ps'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of unique categories\n",
    "unique_categories = np.unique(SupportDataframe_eng.category)\n",
    "unique_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = SupportDataframe_eng['description']\n",
    "categories = SupportDataframe_eng['category_flag']\n",
    "XTrain,XTest,YTrain,YTest = train_test_split(descriptions,\n",
    "                                             categories,\n",
    "                                             stratify = categories,\n",
    "                                             test_size = 0.2,\n",
    "                                             random_state = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FROM TEXTS TO VECTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize Word2Vec model for embedding words to vectors\n",
    "NDim = 100\n",
    "Imdb_w2v = Word2Vec(size = NDim,min_count = 10)\n",
    "Imdb_w2v.build_vocab(XTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8546025, 12531845)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Imdb_w2v.train(XTrain,total_examples = Imdb_w2v.corpus_count,epochs = Imdb_w2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding training messages to vectors for neutral classifier\n",
    "TrainVecs = np.concatenate([buildWordVector(i,NDim,Imdb_w2v) for i in XTrain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.50773859,  0.4288903 , -0.40971846, ..., -0.67132317,\n",
       "         0.92387551, -0.16837773],\n",
       "       [ 0.34783289,  0.43015106, -0.08246623, ..., -0.89399285,\n",
       "         1.21067714, -0.4401564 ],\n",
       "       [ 0.56721134,  0.19724233, -0.13991912, ..., -0.22544182,\n",
       "         0.14983015,  0.17726802],\n",
       "       ...,\n",
       "       [ 0.58387975,  0.39541373, -0.01756927, ..., -0.74149539,\n",
       "         0.71712793,  0.15895999],\n",
       "       [ 0.89165027,  0.51317344, -0.06418519, ..., -0.5327883 ,\n",
       "         0.57747167,  0.08593015],\n",
       "       [ 0.28309082,  0.15320466, -0.34378317, ..., -0.76837949,\n",
       "         0.74710338, -0.15036911]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2100264, 3086620)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Imdb_w2v.train(XTest, total_examples = Imdb_w2v.corpus_count, epochs = Imdb_w2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestVecs = np.concatenate([buildWordVector(i,NDim,Imdb_w2v) for i in XTest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CROSSVALIDATION AND BUILD CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextClassifier = MLPClassifier(hidden_layer_sizes = (20,10), max_iter = 1000, random_state = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(20, 10), max_iter=1000, random_state=40)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextClassifier.fit(TrainVecs,YTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 477  160]\n",
      " [ 238 2326]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.75      0.71       637\n",
      "           1       0.94      0.91      0.92      2564\n",
      "\n",
      "    accuracy                           0.88      3201\n",
      "   macro avg       0.80      0.83      0.81      3201\n",
      "weighted avg       0.88      0.88      0.88      3201\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = TextClassifier.predict(TestVecs)\n",
    "print(confusion_matrix(YTest,pred))\n",
    "print(classification_report(YTest,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0       0.81      0.69      0.75       510\n",
      "           1       0.93      0.96      0.94      2051\n",
      "    accuracy                           0.91      2561 \n",
      "\n",
      "           0       0.80      0.77      0.78       510\n",
      "           1       0.94      0.95      0.95      2051\n",
      "    accuracy                           0.92      2561 \n",
      "\n",
      "           0       0.82      0.71      0.76       510\n",
      "           1       0.93      0.96      0.95      2050\n",
      "    accuracy                           0.91      2560 \n",
      "\n",
      "           0       0.77      0.75      0.76       509\n",
      "           1       0.94      0.95      0.94      2051\n",
      "    accuracy                           0.91      2560 \n",
      "\n",
      "           0       0.80      0.77      0.79       509\n",
      "           1       0.94      0.95      0.95      2051\n",
      "    accuracy                           0.92      2560 \n",
      "\n",
      "[0.91, 0.92, 0.91, 0.91, 0.92]\n",
      "0.914\n"
     ]
    }
   ],
   "source": [
    "# TextClassifier = MLPClassifier(hidden_layer_sizes = (1,), \n",
    "#                                learning_rate_init=0.01,\n",
    "#                                max_iter = 15, \n",
    "#                                random_state = 40)\n",
    "# >> scores 5 folds: [0.9, 0.91, 0.9, 0.91, 0.91]\n",
    "# >> mean: 0.906\n",
    "#\n",
    "# Mark: enougth 1 neuron with 15 iterations to reach high accuracy (1% lower than max reached by me) with nice prec/recll/f1\n",
    "\n",
    "TextClassifier = MLPClassifier(hidden_layer_sizes = (9, 3),\n",
    "                               alpha=0.2,\n",
    "                               learning_rate_init=0.002,\n",
    "                               max_iter = 130, \n",
    "                               random_state = 40)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=40)\n",
    "skf.get_n_splits(TrainVecs, YTrain)\n",
    "\n",
    "folding_reports = []\n",
    "\n",
    "for train_index, test_index in skf.split(TrainVecs, YTrain):\n",
    "    XTrain_fold = TrainVecs[train_index]\n",
    "    YTrain_fold = YTrain.iloc[train_index]\n",
    "    \n",
    "    Xtest_fold = TrainVecs[test_index]\n",
    "    Ytest_fold = YTrain.iloc[test_index]\n",
    "    \n",
    "    TextClassifier.fit(XTrain_fold, YTrain_fold)\n",
    "    fold_pred = TextClassifier.predict(Xtest_fold)\n",
    "    \n",
    "    folding_reports.append(classification_report(Ytest_fold, fold_pred))\n",
    "    \n",
    "    \n",
    "    report = classification_report(Ytest_fold, fold_pred)\n",
    "    print('\\n'.join([row for i, row in enumerate(report.split('\\n')) if i in [2,3,5] ]), '\\n')\n",
    "\n",
    "Scores = [float(report.split('\\n')[5].split()[1]) for report in folding_reports]\n",
    "# Scores = cross_val_score(TextClassifier, TrainVecs, YTrain, cv = 5, n_jobs=-1)\n",
    "print(Scores)\n",
    "print(np.mean(Scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
